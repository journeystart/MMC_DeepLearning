# 常见的优化算法

深度学习中的优化算法经历了`SGD -> SGDM -> NAG -> AdaGrad -> AdaDelta -> Adam -> Nadam`的发展历程。在此进行总结梳理：

## 1、优化过程

定义待优化参数：$\omega$，目标函数：$f(\omega)$，初始学习率：$\alpha$

而后，开始迭代优化，在每个**epoch**中：

①计算目标函数关于当前参数的梯度：**$g_t=\triangledown f(\omega _t)$**

②根据历史梯度计算一阶动量和二阶动量：**$m_t=\phi (g_1,g_2,...,g_t)$；$V_t=\psi(g_1,g_2,...,g_t)$**

③计算当前时刻的下降梯度：**$\eta _t=\alpha \cdot m_t / \sqrt{V_t} $**

④根据下降梯度进行参数更新：**$\omega _{t+1}=\omega_t-\eta_t$**

所有的优化过程都是在上述框架下进行优化的，步骤③④对于各个算法都是一致的，主要的差别体现在①和②上。

## 2、SGD（随机梯度下降）

SGD无动量的概念，因此，$m_t= g_t$，$V_t=I^2$ ($I$为单位矩阵)。

代入步骤3，可以看到下降梯度就是最简单的：**$\eta_t=\alpha \cdot g_t$**

SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。

## 3、SGDM（SGD with momentum）

为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性，下坡的时候，如果发现是陡坡，那就利用惯性跑的快一点。

一阶动量：$m_t=\beta_1 \cdot m_{t-1}+(1-\beta_1) \cdot g_t$

一阶动量是各个时刻，梯度方向的指数移动平均值，时间越近的权重越高。

$\beta_1$的经验值为0.9。

## 4、NAG（SGD with Nesterov Acceleration）

对梯度的计算进行了改进。我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。

因此，NAG在步骤①，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：$g_t= \triangledown f(\omega _t - \alpha \cdot m_{t-1}/ \sqrt{V_{t-1}})$

 **NAG的往前看一小步主要是解决momentum梯度下降冲过头的情况而主要不是为了解决局部最优问题**

## 5、AdaGrad

二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：

$V_t = \sum_{\tau=1}^t {g_t}^2$

梯度的平方可以用来表示梯度的方差，由于方差计算公式为平方的均值减去均值的平方，因此可以用平方的均值粗略反映方差的大小。

$\eta_t = \alpha \cdot m_t / \sqrt{V_t}$ 相当于对学习率进行了自适应缩放，将$\alpha$变为$\alpha / \sqrt{V_t}$

若梯度波动较大，方差变大，$V_t$变大，学习率变小，从而找到更加稳定的训练解；

若梯度波动较小，方差变小，$V_t$变小，学习率变大，从而更快的收敛；

为避免分母为0，加入一个小的平滑项$\eta_t = \alpha \cdot m_t / (\sqrt{V_t} + \epsilon )$     $\epsilon$：防止除零的小常数（如 $10^{-8}$)。

但是，由于$\sqrt{V_t}$是单调递增的，会使得学习率单调递减至0，导致可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。

## 6、AdaDelta

对二阶累积动量也采用指数移动平均值的方法计算：

$V_t=\beta_2 \cdot V_{t-1}+(1-\beta_2) \cdot {g_t}^2$

这样就避免了二阶动量持续累积、导致训练过程提前结束的问题了。

## 7、Adam

SGD的一阶动量：

$m_t=\beta_1 \cdot m_{t-1}+(1-\beta_1) \cdot g_t$

加上AdaDelta的二阶动量：

$V_t=\beta_2 \cdot V_{t-1}+(1-\beta_2) \cdot {g_t}^2$

优化算法里最常见的两个超参数$\beta_1 ,\beta_2$就都在这里了，前者控制一阶动量，后者控制二阶动量。

## 8、Nadam

在Adam的基础上添加上 NAG。

梯度计算：

$g_t= \triangledown f(\omega _t - \alpha \cdot m_{t-1}/ \sqrt{V_{t-1}})$

SGD的一阶动量：

$m_t=\beta_1 \cdot m_{t-1}+(1-\beta_1) \cdot g_t$

加上AdaDelta的二阶动量：

$V_t=\beta_2 \cdot V_{t-1}+(1-\beta_2) \cdot {g_t}^2$

## 9、总结

**算法固然美好，数据才是根本。**

Adam虽然说已经简化了调参，但是并没有一劳永逸地解决问题，默认参数虽然好，但也不是放之四海而皆准。因此，在充分理解数据的基础上，依然需要根据数据特性、算法特性进行充分的调参实验，找到自己优化的最优解。

### 优化常用tricks

1. **首先，各大算法孰优孰劣并无定论。**如果是刚入门，**优先考虑** **SGD+ Nesterov Momentum**或者**Adam.**
2. **选择你熟悉的算法**——这样你可以更加熟练地利用你的经验进行调参。
3. **充分了解你的数据**——如果模型是非常稀疏的，那么优先考虑自适应学习率的算法。
4. **根据你的需求来选择**——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。
5. **先用小数据集进行实验。**有论文研究指出，SGD算法的收敛速度和数据集的大小的关系不大。因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。
6. **考虑不同算法的组合。**先用Adam进行快速下降，而后再换到SGD进行充分的调优。切换策略可以参考本文介绍的方法。
7. **数据集一定要充分的打散（shuffle）。**这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。
8. 训练过程中**持续监控训练数据和验证数据**上的目标函数值以及精度或者AUC等指标的变化情况。对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。
9. **制定一个合适的学习率衰减策略。**可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。

## 10、使用

在正式使用时，由于前期训练时没有历史项，导致被低估，造成系统性偏小，因此，我们使用无偏估计，修正一阶动量和二阶动量。

${1 - \beta_1^t},{1 - \beta_2^t}$作为低估量，我们可以把这个“低估”反过来除掉：
$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$
这就是 Adam 中的 **偏差修正项（bias correction term）**。

- $\hat{m}_t$ 就是对 $m_t$ 的“无偏估计”；
- $\hat{v}_t$ 就是对 $v_t$ 的“无偏估计”。

**带上修正项后，Adam 的更新规则为：**
$$
\theta_{t+1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$
其中：

- $\alpha$：基础学习率
- $\epsilon$：防止除零的小常数（如 $10^{-8}$）

### torch.optim.Adam

PyTorch 中 `torch.optim.Adam `的关键参数：

`params`：要优化的参数（如模型的 parameters()）。

`lr`：学习率（默认值为 0.001）。

`betas`：一阶和二阶动量的衰减系数（默认值为 (0.9, 0.999)）。

`eps`：防止除零的微小值（默认值为 1e-8）。

`weight_decay`：权重衰减系数，用于 L2 正则化。

###  torch.optim.Adam的使用流程

**（1）初始化优化器**

```python
optimizer = torch.optim.Adam(list(actor_model.parameters()) + list(critic_model.parameters()), lr=3e-4)
```

将需要优化的参数列表传入优化器，如`actor_model.parameters()` 和 `critic_model.parameters()`。

可以为不同模块设置不同的学习率。

**（2）梯度清零**

```python
optimizer.zero_grad()
```

每次反向传播前需要梯度清零，避免梯度累加导致错误的更新。

**（3）反向传播**

```python
total_loss.backward()
```

对 `total_loss` 求梯度，梯度会存储在每个参数的 `.grad` 属性中。

**（4）更新参数**

```python
optimizer.step()
```

根据梯度和优化公式更新参数。





